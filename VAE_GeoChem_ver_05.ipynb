{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE_GeoChem_ver_05.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## loading data"
      ],
      "metadata": {
        "id": "0MzO7kUKaAPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn2EH6e6aShn",
        "outputId": "8a42142d-87c3-4951-b3d1-9a065fdbec2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "from keras.utils import plot_model"
      ],
      "metadata": {
        "id": "ocV6zUlyaNyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_frPRni8Z3jv"
      },
      "outputs": [],
      "source": [
        "filenames = np.array(glob(os.path.join(DATA_FOLDER, '*/*.jpg')))\n",
        "NUM_IMAGES = len(filenames)\n",
        "print(\"Total number of images : \" + str(NUM_IMAGES))\n",
        "# prints : Total number of images : 202599\n",
        "\n",
        "\n",
        "INPUT_DIM = (128,128,3) # Image dimension\n",
        "BATCH_SIZE = 512\n",
        "Z_DIM = 200 # Dimension of the latent vector (z)\n",
        "\n",
        "data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(DATA_FOLDER, \n",
        "                                                                   target_size = INPUT_DIM[:2],\n",
        "                                                                   batch_size = BATCH_SIZE,\n",
        "                                                                   shuffle = True,\n",
        "                                                                   class_mode = 'input',\n",
        "                                                                   subset = 'training'\n",
        "                                                                   )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model arch.\n",
        "## Building the Encoder"
      ],
      "metadata": {
        "id": "7A-pH0Bxacx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ENCODER\n",
        "def build_encoder(input_dim, output_dim, conv_filters, conv_kernel_size, \n",
        "                  conv_strides):\n",
        "  \n",
        "  # Clear tensorflow session to reset layer index numbers to 0 for LeakyRelu, \n",
        "  # BatchNormalization and Dropout.\n",
        "  # Otherwise, the names of above mentioned layers in the model \n",
        "  # would be inconsistent\n",
        "  global K\n",
        "  K.clear_session()\n",
        "  \n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  encoder_input = Input(shape = input_dim, name = 'encoder_input')\n",
        "  x = encoder_input\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2D(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'encoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "\n",
        "      x = LeakyReLU()(x)\n",
        "    \n",
        "  # Required for reshaping latent vector while building Decoder\n",
        "  shape_before_flattening = K.int_shape(x)[1:] \n",
        "  \n",
        "  x = Flatten()(x)\n",
        "\n",
        "  # Define model output\n",
        "  encoder_output = Dense(output_dim, name = 'encoder_output')(x)\n",
        "\n",
        "  return encoder_input, encoder_output, shape_before_flattening, Model(encoder_input, encoder_output)\n",
        "\n",
        "encoder_input, encoder_output,  shape_before_flattening, encoder  = build_encoder(input_dim = INPUT_DIM,\n",
        "                                    output_dim = Z_DIM, \n",
        "                                    conv_filters = [32, 64, 64, 64],\n",
        "                                    conv_kernel_size = [3,3,3,3],\n",
        "                                    conv_strides = [2,2,2,2])\n"
      ],
      "metadata": {
        "id": "ulLoQMtRae9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Decoder"
      ],
      "metadata": {
        "id": "la_D9pELaqEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "def build_decoder(input_dim, shape_before_flattening, conv_filters, conv_kernel_size, \n",
        "                  conv_strides):\n",
        "\n",
        "  # Number of Conv layers\n",
        "  n_layers = len(conv_filters)\n",
        "\n",
        "  # Define model input\n",
        "  decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n",
        "\n",
        "  # To get an exact mirror image of the encoder\n",
        "  x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "  x = Reshape(shape_before_flattening)(x)\n",
        "\n",
        "  # Add convolutional layers\n",
        "  for i in range(n_layers):\n",
        "      x = Conv2DTranspose(filters = conv_filters[i], \n",
        "                  kernel_size = conv_kernel_size[i],\n",
        "                  strides = conv_strides[i], \n",
        "                  padding = 'same',\n",
        "                  name = 'decoder_conv_' + str(i)\n",
        "                  )(x)\n",
        "      \n",
        "      # Adding a sigmoid layer at the end to restrict the outputs \n",
        "      # between 0 and 1\n",
        "      if i < n_layers - 1:\n",
        "        x = LeakyReLU()(x)\n",
        "      else:\n",
        "        x = Activation('sigmoid')(x)\n",
        "\n",
        "  # Define model output\n",
        "  decoder_output = x\n",
        "\n",
        "  return decoder_input, decoder_output, Model(decoder_input, decoder_output)\n",
        "\n",
        "decoder_input, decoder_output, decoder = build_decoder(input_dim = Z_DIM,\n",
        "                                        shape_before_flattening = shape_before_flattening,\n",
        "                                        conv_filters = [64,64,32,3],\n",
        "                                        con\n",
        "                                        \n",
        "                                        v_kernel_size = [3,3,3,3],\n",
        "                                        conv_strides = [2,2,2,2]\n",
        "                                        )"
      ],
      "metadata": {
        "id": "FKapc_OIaz_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attaching the Decoder to the Encoder\n"
      ],
      "metadata": {
        "id": "7ltEncRWa39f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The input to the model will be the image fed to the encoder.\n",
        "simple_autoencoder_input = encoder_input\n",
        "\n",
        "# The output will be the output of the decoder. The term - decoder(encoder_output) \n",
        "# combines the model by passing the encoder output to the input of the decoder.\n",
        "simple_autoencoder_output = decoder(encoder_output)\n",
        "\n",
        "# Input to the combined model will be the input to the encoder.\n",
        "# Output of the combined model will be the output of the decoder.\n",
        "simple_autoencoder = Model(simple_autoencoder_input, simple_autoencoder_output)\n",
        "\n",
        "simple_autoencoder.summary()"
      ],
      "metadata": {
        "id": "OBaEDhMia3Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "MD6Up-n8a9zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "N_EPOCHS = 10\n",
        "\n",
        "optimizer = Adam(lr = LEARNING_RATE)\n",
        "\n",
        "def r_loss(y_true, y_pred):\n",
        "    return K.mean(K.square(y_true - y_pred), axis = [1,2,3])\n",
        "\n",
        "simple_autoencoder.compile(optimizer=optimizer, loss = r_loss)\n",
        "\n",
        "checkpoint_ae = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'AE/weights.h5'), save_weights_only = True, verbose=1)\n",
        "\n",
        "simple_autoencoder.fit_generator(data_flow, \n",
        "                                 shuffle=True, \n",
        "                                 epochs = N_EPOCHS, \n",
        "                                 initial_epoch = 0, \n",
        "                                 steps_per_epoch=NUM_IMAGES / BATCH_SIZE,\n",
        "                                 callbacks=[checkpoint_ae])"
      ],
      "metadata": {
        "id": "FtPaY3Wja89A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reconstruction"
      ],
      "metadata": {
        "id": "KBRa16HdidBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "example_batch = next(data_flow)\n",
        "example_batch = example_batch[0]\n",
        "example_images = example_batch[:10]\n",
        "\n",
        "def plot_compare(images=None, add_noise=False):\n",
        "  \n",
        "  if images is None:\n",
        "    example_batch = next(data_flow)\n",
        "    example_batch = example_batch[0]\n",
        "    images = example_batch[:10]\n",
        "\n",
        "  n_to_show = images.shape[0]\n",
        "\n",
        "  if add_noise:\n",
        "    encodings = encoder.predict(images)\n",
        "    encodings += np.random.normal(0.0, 1.0, size = (n_to_show,200))\n",
        "    reconst_images = decoder.predict(encodings)\n",
        "\n",
        "  else:\n",
        "    reconst_images = simple_autoencoder.predict(images)\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 3))\n",
        "  fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "\n",
        "  for i in range(n_to_show):\n",
        "      img = images[i].squeeze()\n",
        "      sub = fig.add_subplot(2, n_to_show, i+1)\n",
        "      sub.axis('off')        \n",
        "      sub.imshow(img)\n",
        "\n",
        "  for i in range(n_to_show):\n",
        "      img = reconst_images[i].squeeze()\n",
        "      sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n",
        "      sub.axis('off')\n",
        "      sub.imshow(img)  \n",
        "\n",
        "plot_compare(example_images)"
      ],
      "metadata": {
        "id": "Pybpa8Fsifyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}